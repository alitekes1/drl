{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=datasets.FashionMNIST(download=True,train=True,root=\"data\",transform=ToTensor())\n",
    "test_data=datasets.FashionMNIST(download=True,train=False,root=\"data\",transform=ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x77b4e71aba60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_sample=train_data[0][0]\n",
    "plt.imshow(data_sample.squeeze(),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader=DataLoader(dataset=train_data,batch_size=32,shuffle=True)\n",
    "test_data_loader=DataLoader(dataset=test_data,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class oluşturmaya kadar gereken işlemler tamamlandı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionModel_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_shape=1\n",
    "        output_shape=10\n",
    "        hidden_unit=10\n",
    "        self.cnn_layer_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,out_channels=hidden_unit,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_unit,out_channels=hidden_unit,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.cnn_layer_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_unit,out_channels=hidden_unit,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_unit,out_channels=hidden_unit,kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )   \n",
    "        self.fully_connected_layer=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_unit*7*7,out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        x=self.cnn_layer_1(x)\n",
    "        x=self.cnn_layer_2(x)\n",
    "        x=self.fully_connected_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionModel_CNN(\n",
       "  (cnn_layer_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (cnn_layer_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fully_connected_layer): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=FashionModel_CNN().to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimazer=torch.optim.SGD(lr=0.01,params=model.parameters())\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 1.30188 | Test loss: 0.00100, Test acc: 78.86%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.50948 | Test loss: 0.00063, Test acc: 82.88%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.44542 | Test loss: 0.00059, Test acc: 84.49%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.40976 | Test loss: 0.00074, Test acc: 80.82%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.38562 | Test loss: 0.00067, Test acc: 85.75%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.36577 | Test loss: 0.00059, Test acc: 84.44%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.35274 | Test loss: 0.00066, Test acc: 86.99%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.33969 | Test loss: 0.00056, Test acc: 86.36%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.32950 | Test loss: 0.00047, Test acc: 86.90%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.31958 | Test loss: 0.00052, Test acc: 87.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "device=\"cuda\"\n",
    "train_loss=0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    model.train()\n",
    "    for batch,(X,y) in enumerate(train_data_loader):\n",
    "        model.train() # modeli train moda aldık\n",
    "        X, y = X.to(\"cuda\"), y.to(\"cuda\")# tüm inputların aynı device üzerinde olması gerekiyor.\n",
    "        y_pred=model(X)\n",
    "        loss=loss_function(y_pred,y)\n",
    "        train_loss+= loss\n",
    "        optimazer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_data_loader.dataset)} samples\")\n",
    "\n",
    "    train_loss /= len(train_data_loader)\n",
    "\n",
    "    model.eval()# bu durumda dropuout ve batchnorm gibi işlemler devre dışı bırakılır.\n",
    "    with torch.inference_mode():# sadece forward propagation yapar. gradiant hesaplanmaz bu sayede parametreler güncellenmez ve bellek optimizasyonu sağlanır.\n",
    "        test_loss,test_accurancy,test_acc=0,0,0\n",
    "        for X,y in test_data_loader:\n",
    "            X, y = X.to(\"cuda\"), y.to(\"cuda\")\n",
    "            test_pred=model(X)\n",
    "            test_loss=loss_function(test_pred,y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        test_loss /= len(test_data_loader)\n",
    "        test_acc /= len(test_data_loader)\n",
    "\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Adımları\n",
    "- ilk önce x ve y değişkenini model ile aynı device a eklememiz gerekiyor.\n",
    "- modelden bir predict yapmasını istiyoruz.\n",
    "- predict ile true değeri loss function a veriyoruz.\n",
    "- optim.zero_grad()\n",
    "- loss.backward() ile back propagation yapıyoruz.\n",
    "- optim.step() ile back propagation sonucu ile weightleri güncelliyoruz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluate Adımları\n",
    "- model.eval() durumunda geçilir\n",
    "- gradient hesaplamalarını yapmamak için torch.with_inference() ın içinde işlemler yapılır\n",
    "- for dongusu ile test_data_loader üzerinde gezinilir. X ve y elde edilir.\n",
    "- traindeki gibi x ve y aynı device e eklenir ardından model tahmin yapar ve loss oluşturulur.\n",
    "- accurancy function a y_pred ve y_test gonderilerek accurancy hesaplanır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
